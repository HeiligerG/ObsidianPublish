---
{"dg-publish":true,"permalink":"/projects/home-assistent/rbpi-5/full-doku/"}
---

# Leistungsfähige KI-Lösung für Raspberry Pi 5 – Empfehlungen und Umsetzung

**Zusammenfassung:** Sie können einen **offenen Sprachassistenten** auf dem Raspberry Pi 5 realisieren, der **personalisierbar**, **zweisprachig (Deutsch/Englisch)**, **offline (keine Inhaltsfilter)** und in ein **Smart-Home** integrierbar ist. Dafür benötigt man:

- Ein **lokal laufendes KI-Sprachmodell** (LLM) mit ausreichend kleiner Größe (z.B. 7 Milliarden Parameter, quantisiert), das auf dem Pi läuft und frei konfigurierbar ist (für **eigenen Namen** und **Antwortstil**).
- Eine **Speech-to-Text (STT)** Engine (zur Spracherkennung Deutsch/Englisch) wie _Vosk_ oder _Whisper_ auf dem Pi ([VOSK Offline Speech Recognition API](https://alphacephei.com/vosk/#:~:text=1,but%20there%20are%20much%20bigger)).
- Eine **Text-to-Speech (TTS)** Engine (zur Sprachausgabe mit natürlicher Stimme) wie _Piper/Mimic 3_, optimiert für Raspberry Pi ([Best Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top Choice? : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=Hey%20everyone%21%20I%27ve%20scoured%20the,that%20you%20can%20test%20here)) ([GitHub - rhasspy/piper: A fast, local neural text to speech system](https://github.com/rhasspy/piper#:~:text=A%20fast%2C%20local%20neural%20text,in%20a%20variety%20of%20projects)).
- Eine **Integrationsplattform** – am einfachsten **Home Assistant** mit dem neuen Sprachassistent-Framework (oder alternativ ein offline-Voice-Projekt wie _Rhasspy_), um die KI mit Smart-Home-Geräten zu verbinden ([Rhasspy](https://rhasspy.readthedocs.io/#:~:text=Rhasspy%20,5%20that%20works%20well%20with)).

Im Folgenden gehen wir auf die empfohlenen Modelle/Frameworks, die Einrichtung und die geeignete Softwareumgebung im Detail ein.

## KI-Modell: Auswahl eines anpassbaren LLM für den Raspberry Pi

Für frei formulierte Antworten **ohne inhaltliche Einschränkungen** bietet sich ein **offenes Large Language Model** an, das lokal läuft. Wichtig ist dabei die **Modellgröße**: Ein Raspberry Pi 5 (vorzugsweise mit **8 GB RAM**) kann ein Modell mit ~6–7 Milliarden Parametern gerade noch stemmen, wenn es effizient komprimiert ist ([Just for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry Pi 4 : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited)) ([Running LLM Models (LLaMA, Alpaca, LLaMA2, ChatGLM) on Raspberry Pi 4B for Edge Computing - DFRobot](https://www.dfrobot.com/blog-13412.html?srsltid=AfmBOooNjqON_epINpbvnhk4T0QJ0nTjjpX6RPoDUOrJg1nY4qBqxvya#:~:text=Large%20language%20models%20usually%20specify,LLMs%20on%20the%20Raspberry%20Pi)). Größere Modelle (13B, 30B…) wären zu ressourcenhungrig. Empfohlene Optionen:

- **Meta LLaMA 2 (7B)** oder davon abgeleitete Modelle: LLaMA 2 ist ein leistungsfähiges Basismodell, das auch für deutschsprachige Eingaben verwendet werden kann. In quantisierter Form (z.B. 4-Bit-Quantisierung) benötigt ein 7B-Modell rund 5 GB RAM und läuft auf dem Pi 5, wenn auch nur mit begrenzter Geschwindigkeit ([Just for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry Pi 4 : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited)). Die Antworten kommen **ohne vorgeschriebene Filter** (Sie steuern die KI komplett selbst). Durch Anpassung der **Prompts** (Systemnachricht) können Sie der KI einen Namen geben und den Ton der Antworten festlegen. Gegebenenfalls lässt sich LLaMA 2 auf Ihrem PC (RTX 3070 Ti) **feintunen** oder mittels Low-Rank Adaption (LoRA) an einen gewünschten **Stil** anpassen, und anschließend das feingetunte Modell auf dem Pi ausführen.
    
- **Open-Assistant/Anthropic-ähnliche Modelle (z.B. OpenAssistant 7B)**: Das Projekt **OpenAssistant** hat frei verfügbare Dialogmodelle hervorgebracht, die auf Nutzeranfragen antworten können. Diese Modelle wurden auf community-generierten Dialogen (teils mehrsprachig) trainiert und besitzen keine harten Zensurfilter. Ein 7B-Modell von OpenAssistant kann vergleichbar wie LLaMA 2 auf dem Pi laufen (ebenfalls mit ~5 GB RAM Bedarf nach Quantisierung). Vorteil: Sie sind darauf ausgelegt, **Anweisungen** zu befolgen und könnten natürlicher auf Fragen reagieren („nach bestem Wissen“). Alternativ gibt es weitere **RLHF-freie** Modelle (teils als _“uncensored”_-Variante bezeichnet), die bewusst ohne Inhaltsbeschränkung trainiert wurden – diese ließen sich ebenfalls nutzen, erfordern aber sorgfältigen Umgang.
    
- **Multilinguale Modelle (z.B. Teuken-7B)**: Da Sie Deutsch _und_ Englisch benötigen, lohnt sich ein Modell, das in beiden Sprachen kompetent ist. Viele neuere Open-Source-LLMs sind **mehrsprachig** trainiert. Ein Beispiel ist _Teuken-7B_ aus dem deutschen **OpenGPT-X** Projekt – ein 7-Mrd.-Parameter-Modell, das **in allen 24 EU-Sprachen (inkl. Deutsch und Englisch)** von Grund auf trainiert wurde ([Multilingual and open source: OpenGPT-X research project releases large language model – Fraunhofer Audio Blog](https://www.audioblog.iis.fraunhofer.com/open-gptx-llm#:~:text=The%20large%20language%20model%20of,and%20has%20a%20distinctly%20European)) ([Multilingual and open source: OpenGPT-X research project releases large language model – Fraunhofer Audio Blog](https://www.audioblog.iis.fraunhofer.com/open-gptx-llm#:~:text=Teuken,can%20remain%20within%20the%20company)). Solche Modelle bringen eine _“europäische”_ Perspektive mit und sind kommerziell nutzbar. Ein mehrsprachiges Modell wird Ihre deutschsprachigen Fragen natürlicher verstehen und beantworten. Alternativ haben auch viele englische Basismodelle einen gewissen Anteil deutscher Trainingsdaten, sodass einfache Fragen auf Deutsch oft korrekt beantwortet werden. Zur Sicherheit kann man das gewählte Modell ggf. mit einigen deutschen Beispiel-Dialogen nachtrainieren (auf dem PC), um den Stil und die Verständigkeit weiter zu verbessern.
    

**Leistung und Anpassung:** Egal für welches Modell Sie sich entscheiden, der Workflow wäre: **Modellbeschaffung und -anpassung auf dem PC**, dann **Deployment auf dem Pi**. Beispielsweise können Sie auf dem PC das Modell herunterladen und mit Tools wie _llama.cpp_ oder _GPT4All_ in ein quantisiertes Format konvertieren (4-bit INT4 oder 8-bit) ([Running LLM Models (LLaMA, Alpaca, LLaMA2, ChatGLM) on Raspberry Pi 4B for Edge Computing - DFRobot](https://www.dfrobot.com/blog-13412.html?srsltid=AfmBOooNjqON_epINpbvnhk4T0QJ0nTjjpX6RPoDUOrJg1nY4qBqxvya#:~:text=with%20smaller%20memory%20footprints,LLMs%20on%20the%20Raspberry%20Pi)). Testen Sie es zuerst am PC, um sicherzustellen, dass die gewünschten Persona-Einstellungen (Name, keine Zensur, Antwortstil) funktionieren. Anschließend übertragen Sie das kompakte Modell auf den Raspberry Pi. Beachten Sie, dass selbst mit Pi 5 die Antwortzeiten signifikant länger sind als bei PC/GPU – eine 7B-KI kann auf dem Pi einige Sekunden pro Wort benötigen ([Just for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry Pi 4 : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited)). Falls Ihnen das zu langsam ist, gibt es zwei Ansätze: (1) **noch kleinere Modelle** einsetzen (z.B. 2–3 Billionen Parameter wie _TinyLlama_ oder _GPT-J 6B_), die schneller laufen, oder (2) **Rechenlast aufteilen** – z.B. den LLM-Teil auf dem PC laufen lassen und nur STT/TTS auf dem Pi (wenn ein stets verfügbarer PC nicht stört). Für völlige Autarkie bleibt jedoch ein quantisiertes 7B-Modell auf dem Pi die beste Kombination aus Leistung und Qualität.

**Inhaltliche Freiheiten:** Da die KI offline und unter Ihrer Kontrolle läuft, können Sie _sämtliche_ inhaltlichen Filter entfernen oder anpassen. Kommerzielle Assistenten verweigern oft Antworten aus ethischen Gründen – Ihre eigene KI hingegen kann _nach bestem Wissen und ohne Einschränkungen_ antworten, solange das zugrundeliegende Modell die Information enthält. Achten Sie aber darauf, verantwortungsvoll mit dieser Freiheit umzugehen. Sie können in den Systemeinstellungen der KI (Initial-Prompt) definieren, dass die KI z.B. **offen und direkt** antworten soll, und dass sie keine vordefinierten moralischen Belehrungen geben muss. So erhalten Sie die gewünschten unzensierten Antworten.

## Sprachverarbeitung: STT (Spracherkennung) und TTS (Sprachausgabe)

Für eine echte Sprachassistent-Erfahrung müssen Spracheingaben in Text umgewandelt und Antworten wieder vorgelesen werden. Hier bewähren sich **offline-fähige STT/TTS-Lösungen** auf dem Raspberry Pi:

- **Speech-to-Text (Spracherkennung)**: Ein sehr gutes offline STT-Toolkit ist **Vosk** (basierend auf Kaldi). Vosk bietet vortrainierte Akustikmodelle für **über 20 Sprachen, darunter Deutsch und Englisch**, und läuft selbst auf schwacher Hardware **komplett offline (auch auf Raspberry Pi)** ([VOSK Offline Speech Recognition API](https://alphacephei.com/vosk/#:~:text=1,but%20there%20are%20much%20bigger)). Sie können also mit einem deutschen oder englischen Satz ins Mikro sprechen, und Vosk liefert Ihnen den erkannten Text. Die Genauigkeit für deutschsprachige Befehle oder Fragen ist ordentlich, wenn auch nicht ganz auf dem Niveau moderner Cloud-Dienste – dafür bleiben die Daten lokal. Eine Alternative ist **Whisper** (OpenAI), ein neuronales STT-Modell, das sehr hohe Erkennungsraten erzielt, insbesondere bei Mischsprachen. Allerdings ist Whisper in den größeren Modellvarianten rechenintensiv; auf dem Pi 5 müsste man die **kleinste Version (Tiny oder Small)** verwenden, was evtl. noch funktioniert, aber deutlich langsamer ist als Vosk. (In einer Testumgebung war Whisper **ohne GPU auf dem Pi extrem langsam** ([How to control Home Assistant with a local LLM instead of ChatGPT | The awesome garage](https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=To%20start%20with%2C%20you%20need,GPU%20is%20a%20painful%20experience)).) Für flüssige Interaktion in Echtzeit ist Vosk daher empfehlenswerter. Vosk lässt sich einfach installieren (`pip3 install vosk`) und es stehen kompakte Modelldateien (~50 MB) für jedes Sprache zur Verfügung ([VOSK Offline Speech Recognition API](https://alphacephei.com/vosk/#:~:text=Tajik%2C%20Telugu,different%20programming%20languages%2C%20too)) – ideal für den Pi.
    
- **Text-to-Speech (Sprachausgabe)**: Für die Ausgabe mit einer natürlich klingenden Stimme bietet sich **Piper (Mimic 3)** an. Piper ist eine **neuronale TTS-Engine**, die speziell für _Edge-Geräte_ entwickelt wurde. Sie läuft **lokal in Echtzeit** auf einem Raspberry Pi 4/5 und liefert hochwertige synthetische Stimmen ([Best Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top Choice? : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=Hey%20everyone%21%20I%27ve%20scoured%20the,that%20you%20can%20test%20here)) ([GitHub - rhasspy/piper: A fast, local neural text to speech system](https://github.com/rhasspy/piper#:~:text=A%20fast%2C%20local%20neural%20text,in%20a%20variety%20of%20projects)). Piper ist Teil der Open-Source-Mycroft/OVOS-Community (_Open Voice OS_) und wird aktiv weiterentwickelt (Open Home Foundation, _Year of Voice_ Projekt). Es gibt **vorgefertigte Stimmen für viele Sprachen** – darunter mehrere **deutsche und englische Stimmen** ([GitHub - rhasspy/piper: A fast, local neural text to speech system](https://github.com/rhasspy/piper#:~:text=,English%20%28en_GB%2C%20en_US)), die Sie frei auswählen können. Sie können so die **Stimme der KI** Ihrer Vorliebe anpassen (z.B. eine weibliche deutsche Stimme für „Alexa-ähnliches“ Klangbild, oder eine männliche englische Stimme – je nach Persona). Die Qualität von Piper ist für Offline-TTS beeindruckend: Das System nutzt neuronale Sprachsynthese (VITS-Modell als Basis) und erreicht nahezu natürliches Timbre. In Community-Tests wird Piper 2024 als **führende Offline-TTS-Lösung für den Pi** angesehen („**Piper ist… schnell, für Raspberry Pi optimiert und bietet tolle Stimmen in fast jeder Sprache**“ ([Best Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top Choice? : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=Hey%20everyone%21%20I%27ve%20scoured%20the,that%20you%20can%20test%20here))). Installation und Nutzung sind ebenfalls unkompliziert (es gibt Docker-Container oder man installiert das Python-Paket). Alternativen wären _eSpeak_ (sehr robotic, nur als Notfalllösung) oder _Coqui-TTS_ (auch neuronale Stimmen, aber Piper ist praktisch eine spezialisierte Abspaltung davon). Daher empfiehlt es sich, Piper/Mimic 3 direkt zu verwenden.
    

> **Tipp:** Home Assistant bzw. Rhasspy nutzen Piper bereits als Standard-TTS in ihren Sprachfeatures. Sie können auf der [Piper-Projektseite](https://rhasspy.github.io/piper-samples/) Audiobeispiele verschiedener Stimmen anhören ([Best Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top Choice? : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=best%20solution%20for%202024,that%20you%20can%20test%20here)). Wählen Sie eine Stimme, die Ihnen gefällt, und laden Sie das entsprechende Sprachmodell (für Deutsch z.B. die Stimme _“de_DE/thorsten”_ oder _“de_DE/elaine”_, für Englisch z.B. _“en_US/lessac”_). Diese .onnx-Stimmdateien werden dann vom Piper-Server geladen, um Texte in Sprache umzuwandeln.

- **Sprachumschaltung:** Da Ihre KI zweisprachig agieren soll, müssen STT und TTS **dynamisch die Sprache erkennen bzw. wechseln** können. Vosk-Engine kann mit separaten Modellen pro Sprache betrieben werden – Sie könnten anhand der ersten erkannten Worte entscheiden, ob der Input Deutsch oder Englisch ist, und dann das passende Spracherkennungsmodell verwenden. (Alternativ gibt es Multilingual-Modelle für STT, z.B. Whisper kann Sprache automatisch erkennen – bei Vosk müsste man diesen Schritt selbst vornehmen, z.B. über einen kurzen **Language Identifier**.) Für TTS könnten Sie entweder eine zweisprachige Stimme nutzen (falls verfügbar) oder je nach Sprache der Antwort eine deutsche bzw. englische Stimme wählen. Piper hat z.B. für Englisch und Deutsch separate Stimmen, aber Sie können dem System beibringen: _„Wenn Antworttext Deutsch, dann deutsche Stimme nutzen.“_ Diese Logik lässt sich im Integrations-Code umsetzen. Das KI-Modell selbst wird anhand der Frage vermutlich auch in der entsprechenden Sprache antworten. (Falls nicht, kann man im Prompt vorgeben: _„Beantworte deutsche Fragen auf Deutsch und englische auf Englisch.“_) Insgesamt ist so sichergestellt, dass die KI Sie versteht und in passender Sprache antwortet.

## Personalisierung: Name, Stimme und Antwortstil der KI

Ein großer Vorteil einer eigenen KI-Lösung ist die **freie Personalisierung**. Sie können Ihrer KI einen individuellen **Namen** geben, die **Stimmfarbe** anpassen und den **Tonfall der Antworten** konfigurieren:

- **Name und Anrede:** Der Name der KI ist rein konfigurationsseitig – Sie können ihn frei wählen (z.B. „Ada“, „Jarvis“, „Heimhelfer“, etc.). Im Sprachgebrauch können Sie diesen Namen als **Wake Word** verwenden, falls Sie einen Hotword-Listener einsetzen (z.B. “Hey Jarvis” als Signal zum Zuhören). Offline-Wakeword-Erkennung kann z.B. mit _Porcupine_ oder Mycroft **Precise** realisiert werden, wo Sie ein eigenes Keyword trainieren können. Alternativ nutzen Sie einfach einen Knopfdruck oder Trigger in Home Assistant, um das Mikro zu aktivieren – dann ist der Name mehr für die **Ansprache in Antworten** relevant. Sie können dem KI-Modell im System-Prompt mitteilen: _„Du heißt [Name] und sprichst den Nutzer mit ‚du/Sie‘ an.“_ So kann die KI ihren Namen ggf. in ihren Sätzen verwenden (oder Sie skripten es, dass die KI z.B. sagt „[Name] steht zu Diensten.“ o.ä.). Da das Modell komplett unter Ihrer Kontrolle ist, können Sie solche Persona-Details frei festlegen.
    
- **Stimme:** Die Stimme wird durch das TTS-System bestimmt. Mit Piper/Mimic3 können Sie aus zahlreichen Stimmen auswählen ([GitHub - rhasspy/piper: A fast, local neural text to speech system](https://github.com/rhasspy/piper#:~:text=,English%20%28en_GB%2C%20en_US)). Für eine persönliche Note könnten Sie sogar Ihre _eigene Stimme_ klonen – das erfordert jedoch das Training eines eigenen TTS-Stimmenmodells (mit einigen Minuten Sprachaufnahmen und Training am PC). Einfacher ist die Wahl einer bestehenden sympathischen Stimme. Beispielsweise gibt es eine deutsche männliche Stimme (_Thorsten_ – recht natürlich klingend) und eine weibliche (_Eva_ oder _Birgit_). Für Englisch sind Stimmen wie _LibriVox_ oder _Lessac_ verfügbar. Die Stimme können Sie jederzeit austauschen oder für verschiedene Sprachen unterschiedliche Sprecher nutzen. So bekommt Ihre KI einen eindeutigen Charakterklang.
    
- **Antwortstil:** Der Sprachstil der Antworten lässt sich auf **mehreren Ebenen** anpassen. Erstens können Sie im **Prompting** dem KI-Modell Anweisungen geben, z.B.: _„Antworte stets höflich und fachkundig, aber ruhig locker. Du darfst auch humorvoll sein.“_ oder _„Verwende bei Antworten auf Deutsch eher informelle Sprache (Du) und auf Englisch einen freundlichen Ton.“_ Diese Systeminstruktionen wirken wie eine Persönlichkeitsschablone für das Modell. Zweitens können Sie durch **Fine-Tuning** die KI an einen bestimmten Stil gewöhnen – z.B. indem Sie Beispiel-Dialoge im gewünschten Ton trainieren. Drittens entscheidet auch das konkrete Modell: einige Modelle (z.B. OpenAssistant) neigen zu ausführlichen, möglicherweise lockeren Antworten; andere (z.B. Alpaca) antworten sehr prägnant. Wählen Sie ein Modell, dessen „Stimmcharakter“ Ihren Vorstellungen entspricht, oder passen Sie es an. Da Sie keine Filter haben, können Sie auch kontroverse oder sarkastische Stile zulassen, wenn gewünscht. Wichtig: Falls Sie ein bereits **RLHF-gefeintes** Modell wie LLaMA 2 Chat nutzen, könnten dort noch milde Inhaltsfilter aktiv sein (die Modelle wurden darauf trainiert, beleidigende Inhalte zu vermeiden). Diese können Sie überschreiben, indem Sie im Prompt klarstellen, dass die KI _alles beantworten_ soll. Notfalls greifen Sie zu einem Basismodell ohne RLHF oder einem explizit „ungefilterten“ Community-Finetune.
    

Zusammengefasst ist **Personalisierung** eine Stärke dieser DIY-KI: Von Name über Stimme bis Antwortverhalten können Sie alles Ihren Wünschen entsprechend einstellen. Vieles davon erfordert nur Konfiguration (keine Programmierung) – z.B. den Namen in einem Skript ändern, eine andere TTS-Stimme wählen oder ein paar Beispielsätze als Vorlage geben, damit die KI den gewünschten Ton trifft.

## Integration ins Smart Home: Home Assistant vs. Alternativen

Um die KI praktisch einzusetzen, insbesondere für Smart-Home-Steuerung, braucht es eine **Software-Umgebung**, die **Mikrofon**, **Lautsprecher**, **KI-Logik** und **Haussteuerung** zusammenbringt. Hier bieten sich zwei Hauptwege an:

**1. Home Assistant mit Voice Assistant Pipeline:**

Home Assistant (HA) ist eine mächtige Smart-Home-Zentrale und hat kürzlich umfangreiche Sprachassistenz-Funktionen erhalten (Stichwort _“Year of Voice”_ bei HA). Ab **Home Assistant 2023.x** können Sie eigene **Voice Pipelines** konfigurieren – das heißt, Sie können **frei wählen, welche STT-Engine, KI und TTS-Engine** genutzt werden sollen, sowie ein Wakeword ([How to control Home Assistant with a local LLM instead of ChatGPT | The awesome garage](https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=With%20the%20introduction%20of%20Voice,the%20local%20LLM%20to%20an)). HA liefert damit den Orchestrierungsrahmen: Sobald Sie einen Sprachbefehl geben, wandelt HA ihn per STT in Text, gibt ihn an einen **Conversation Agent** (Ihre KI) weiter, erhält die Antwort als Text und liest sie per TTS vor. Dies alles kann lokal geschehen, wenn die entsprechenden Module lokal angebunden sind. Sie könnten z.B. in der `configuration.yaml` von HA einen **Conversation-Agent** vom Typ „OpenAI“ eintragen, der aber auf einen lokalen Endpoint zeigt (dazu später mehr). Für STT wählen Sie **Vosk** oder **Whisper** (es gibt Community-Addons bzw. Integrationen dafür), für TTS wählen Sie **Piper** (ist in HA inzwischen integriert oder als Add-on verfügbar). Ihr Raspberry Pi 5 könnte all das ausführen – ggf. in Form von Docker-Containern, die HA orchestriert.

Die Integration mit dem Smart Home ist in HA am reibungslosesten: Ihre KI kann direkt **Geräte schalten** oder **Sensoren abfragen**, wenn Sie entsprechende Intents definieren. Es gibt zwei Möglichkeiten, wie die KI mit HA-Devices interagieren kann: (a) über **vordefinierte Absichten** (Intents), die z.B. bestimmte Phrasen („Licht an im Wohnzimmer“) erkennen und dann einen HA-Befehl auslösen, oder (b) über die neue **Funktionen-API** von OpenAI, die Home Assistant unterstützt – dabei kann das KI-Modell sogenannte _function calls_ auslösen. Ein entsprechend trainiertes Modell (oder ein Prompt mit Funktionenschema) könnte z.B. bei der Anfrage _„Schalte das Wohnzimmerlicht auf 50%.“_ intern eine JSON-Funktion `light.turn_on` an HA schicken. Tatsächlich experimentiert die Community mit solchen Ansätzen; im Blog-Beispiel wurde ein **LLM-Modell verwendet, das Home-Assistant-Funktionen versteht** ([How to control Home Assistant with a local LLM instead of ChatGPT | The awesome garage](https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=When%20you%27ve%20gotten%20Whisper%20and,to%20run%20on%20your%20GPU)). Das ist allerdings fortgeschritten – einsteigertauglicher ist es, Intents per HA’s eigenem Intent-System abzubilden und die KI für freies Fragen/Antworten daneben laufen zu lassen.

**Warum Home Assistant?** Wenn Sie ohnehin ein HA-System betreiben, bietet es sich an, dieses als _Zentralplattform_ zu nutzen. HA gewährleistet, dass alle Komponenten (Mikrofon, Lautsprecher, KI, Geräte) zusammenspielen. Zudem bleibt Ihre Lösung **erweiterbar** – Sie können zukünftige HA-Updates nutzen, die evtl. lokale LLM-Integration noch einfacher machen. (Die Entwickler von HA arbeiten aktiv an lokalen Sprachsteuerungsfunktionen.) Im Zweifel lässt sich auch ein Teil der Pipeline auf dem PC auslagern, während HA auf dem Pi läuft – z.B. könnte **Whisper auf dem PC** laufen und HA per Netzwerk dessen STT-Ergebnis nutzen ([How to control Home Assistant with a local LLM instead of ChatGPT | The awesome garage](https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=blazing%20fast%20performance,enough%20to%20follow%20this%20tutorial)), oder das LLM auf dem PC gehostet und HA schickt die Prompts dorthin. HA ist flexibel genug für solche verteilten Setups.

**2. Alternativen: Rhasspy oder andere Offline-Assistant-Frameworks:**

Falls Sie **nicht** das gesamte Home-Assistant-Framework nutzen möchten (oder bereits eine andere Smarthome-Zentrale haben), gibt es spezialisierte **Offline-Sprachassistenten**. **Rhasspy** ist ein bekanntes Open-Source-Projekt, das genau dies bietet: Eine **vollständig Offline-Sprachassistent-Plattform für viele Sprachen**, lauffähig auf Raspberry Pi ([Rhasspy](https://rhasspy.readthedocs.io/#:~:text=Rhasspy%20,5%20that%20works%20well%20with)). Rhasspy wurde als Nachfolger von Snips.ai entwickelt und unterstützt ebenfalls **Home Assistant, Node-RED, OpenHAB** u.a. Systeme out-of-the-box ([Rhasspy](https://rhasspy.readthedocs.io/#:~:text=voice%20assistant%20services%20%20for,5%20that%20works%20well%20with)). Man definiert bei Rhasspy _Sprachbefehle mittels Vorlagen_ (Grammatik), und Rhasspy erkennt diese und sendet JSON-Events, um Aktionen auszulösen ([Rhasspy](https://rhasspy.readthedocs.io/#:~:text=You%20specify%20voice%20commands%20in,a%20template%20language)). Für Ihr Vorhaben würde Rhasspy den Part von STT (z.B. mit eigenen Wakeword, Vosk STT) und TTS (Piper) übernehmen und die Smart-Home-Befehle direkt an Home Assistant weitergeben. **Vorteil:** Sehr ausgereifte Offline-Sprachsteuerung, **Nachteil:** Freie Konversation (offene Fragen an die KI) sind standardmäßig nicht vorgesehen. Allerdings lässt sich Rhasspy erweitern – man kann einen _Fallback-Intent_ definieren, der alle Eingaben auffängt, die keinem Befehl zugeordnet wurden, und diesen Text dann an Ihr KI-Modell schickt. Die KI-Antwort könnte Rhasspy dann wieder per TTS ausgeben. So erhalten Sie neben den vordefinierten Haussteuerungs-Kommandos auch spontane Frage-Antwort-Fähigkeiten. Rhasspy erlaubt viel Feintuning und läuft stabil auf dem Pi; es ist jedoch etwas komplexer in der Ersteinrichtung als HA (man konfiguriert Profile, muss ggf. Audio-Hardware einrichten, etc.). Wenn Sie bereits HA nutzen, ist wahrscheinlich der direkte HA-Weg einfacher. Wenn Sie aber lieber eine **entkoppelte Sprachsteuerung** haben wollen (die notfalls ohne HA-Kern funktioniert) oder mehrere _Sprach-Satelliten_ im Haus (mehrere Pis mit Mics in verschiedenen Räumen) betreiben möchten, dann ist Rhasspy hervorragend geeignet – es unterstützt sogar verteilte Installationen (Satellit/Nachricht broker).

**3. Weitere Optionen:** Neben Rhasspy gibt es **OpenVoiceOS (OVOS)** bzw. **Neon AI**, Forks des Mycroft-Assistenten. Diese bieten komplette Sprachassistenz mit Skill-System. Man könnte z.B. einen OVOS auf dem Pi 5 laufen lassen, der mittels einer _ChatGPT-Skill_ (hier dann auf Ihr lokales LLM umgebogen) allgemeine Fragen beantwortet, und mittels einer _HomeAssistant-Skill_ Geräte steuert. OVOS hat Personalisierungs-Features (kann benannt werden, hat TTS/STT plugins etc.). Allerdings ist Mycroft/OVOS relativ schwergewichtig und noch in Entwicklung – für Bastler aber einen Blick wert. Ebenfalls möglich ist die Nutzung von **Node-RED** Flows, um STT->LLM->TTS Ketten zu bauen, oder Sprachmodule in anderen Smart-Home-Lösungen (OpenHAB z.B. kann externe Skripte aufrufen). Insgesamt scheint jedoch **Home Assistant mit seinen neuen Voice-Fähigkeiten die naheliegendste und zukunftssichere Umgebung** für Ihr Vorhaben zu sein.

## Einrichtung und Anpassung: Vorgehensweise

Zum Abschluss hier ein mögliches **Vorgehensschema**, um die KI auf dem Raspberry Pi 5 zum Laufen zu bringen und ins Smart Home zu integrieren:

1. **Modellvorbereitung auf dem PC:** Installieren Sie auf Ihrem leistungsfähigen PC die benötigten Tools (z.B. `transformers` oder `llama.cpp`), und laden Sie das gewünschte Basismodell herunter (z.B. LLaMA 2 7B oder Teuken-7B). Führen Sie ggf. ein **Finetuning oder LoRA-Merge** durch, um persönliche Anpassungen (z.B. bilinguales Verhalten, kein Filter, bestimmter Gesprächston) einzubringen. Anschließend **quantisieren** Sie das Modell, um es zu verkleinern – etwa mit _GPTQ_ oder _llama.cpp quantize_. Ziel ist ein Modell, das <6 GB Speicher belegt. Testen Sie das quantisierte Modell auf dem PC einmal mit ein paar deutschen und englischen Fragen, um sicher zu sein, dass es korrekt funktioniert und Ihren Anforderungen entspricht (z.B. keine ungewollten Refusals).
    
2. **Raspberry Pi einrichten:** Sorgen Sie dafür, dass der Pi 5 eine geeignete Linux-Distribution laufen hat (z.B. Raspberry Pi OS 64-Bit). Installieren Sie die nötigen Abhängigkeiten für Audio (Mikrofon und Lautsprecher sollten am Pi funktionieren). Kopieren Sie das quantisierte Modell zum Pi. Installieren Sie dann:
    
    - **STT**: z.B. `vosk` Python-Bibliothek und laden Sie das deutsche und englische Modell für Vosk. (Alternativ Whisper.cpp, falls ausprobiert.)
    - **TTS**: z.B. Piper – entweder via Docker (es gibt ein [rhasspy/piper Docker-Image](https://hub.docker.com/r/rhasspy/piper)) oder durch Kompilieren der ONNX Runtime auf ARM. Laden Sie die gewünschten Stimmenpakete (de_DE und en_US).
    - **LLM Runtime**: Installieren Sie `llama.cpp` auf dem Pi (lässt sich für ARM kompilieren) oder nutzen Sie ein leichtgewichtiges Serving-Framework wie **LocalAI**. _LocalAI_ ist ein Open-Source-Projekt, das eine API bereitstellt, welche kompatibel zur OpenAI-API ist, aber lokal Ihre Modelle ausführt ([How to control Home Assistant with a local LLM instead of ChatGPT | The awesome garage](https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=When%20you%27ve%20gotten%20Whisper%20and,to%20run%20on%20your%20GPU)). Damit könnten Sie z.B. Home Assistant vorgaukeln, es spräche mit der OpenAI API, tatsächlich aber antwortet LocalAI mit Ihrem lokalen Modell. LocalAI kann per Docker auf dem Pi laufen (es unterstützt CPU-Betrieb mit quantisierten Modellen). Alternativ können Sie auch ein eigenes Python-Skript schreiben, das Texteingaben an llama.cpp weiterleitet – für HA-Integration ist aber die API-Methode bequemer.
3. **Verknüpfung der Komponenten:** Wenn Sie Home Assistant verwenden, richten Sie dort eine **Conversation Pipeline** ein. Beispiel: Wake Word (optional) -> STT (Vosk) -> Conversation Agent (LocalAI mit Ihrem Modell) -> TTS (Piper). In der HA-Dokumentation bzw. Community finden sich Beispiele, wie man benutzerdefinierte STT/TTS einbindet und einen eigenen Conversational Agent konfiguriert. Oft läuft es darauf hinaus, in den Einstellungen von HA einen externen URL für STT/TTS anzugeben (wo z.B. ein Piper-Server lauscht) und einen **“ChatGPT”**-ähnlichen Agenten zu konfigurieren, der aber auf LocalAI zeigt. Sobald dies steht, können Sie per Sprache mit Ihrem HA-System sprechen. Sagen Sie z.B. _„Wie ist das Wetter heute in Berlin?“_, dann sollte die Pipeline greifen: Ihr Satz wird von Vosk erkannt, als Text an das LLM geschickt, dieses generiert eine Antwort, und Piper gibt die Antwortstimme aus. – **Tipp:** Testen und debuggen Sie schrittweise: Erst STT allein (Mikrofon-Test), dann TTS (eine Testnachricht sprechen lassen), dann die KI-Textantwort via HA’s Text-Eingabe (ohne Sprache) überprüfen, und schließlich alles verkoppeln.
    
4. **Smart-Home-Steuerung einbinden:** Bringen Sie Ihrer KI bei, auch Geräte zu steuern. Der einfache Weg: Definieren Sie in Home Assistant **Sprachbefehle/Intents** für typische Aktionen (Licht, Thermostat, etc.). HA kann solche Befehle auch ohne LLM verstehen (per Regex oder Synonym-Liste). Die KI muss also nicht jeden Befehl selbst „wissen“. Stattdessen fangen die HA-Intents bestimmte Schlüsselwörter ab und schalten direkt die Geräte. Für frei formulierte Befehle können Sie die KI nutzen: Z.B. Frage: _„Mir ist kalt.“_ -> KI erkennt evtl., dass die Heizung höher gestellt werden soll – Sie könnten das KI-Antwort-Template so gestalten, dass sie in solchen Fällen eine Funktion ausführt oder einen speziellen JSON zurückgibt, den HA auswertet. Dies ist allerdings komplex. Einfacher: Entweder nur feste Sprachbefehle fürs Smart Home (und KI nur für generelle Fragen), **oder** KI-Antworten nicht nur als Text behandeln, sondern parsen. Die Home-Assistant-Community entwickelt hier laufend Lösungen, sodass Ihre lokale KI Kontext über Ihr Smart Home hat.
    
5. **Feinschliff und Testing:** Zum Schluss justieren Sie die Persona-Details. Experimentieren Sie mit verschiedenen Prompt-Vorlagen, bis Ihnen der Ton gefällt. Wechseln Sie die TTS-Stimme, falls die Verständlichkeit oder Persönlichkeit noch nicht passt. Testen Sie bilinguale Dialoge – z.B. erst eine Frage auf Deutsch, dann eine auf Englisch – und prüfen Sie, ob die KI korrekt umschaltet. Möglicherweise müssen Sie in Ihrem Pipeline-Code die Sprache erkennen und dem KI-Modell mitteilen. All das können Sie nach und nach verbessern. Da alles lokal läuft, haben Sie die volle Kontrolle und keine Abhängigkeit von Cloud-APIs.
    

**Fazit:** Mit einem Raspberry Pi 5 und der richtigen Auswahl an Open-Source-Komponenten können Sie einen _hochgradig anpassbaren Sprachassistenten_ umsetzen. Ein quantisiertes **LLM (ca. 7 B Parameter)** läuft auf dem Pi und liefert intelligente Antworten in Deutsch und Englisch, **ohne Zensur**. Kombiniert mit **Vosk** (offline-STT) und **Piper** (offline-TTS) bleibt die Lösung komplett lokal. Für die Smart-Home-Integration ist **Home Assistant** empfehlenswert, da es bereits Hooks für STT/TTS und Gerätesteuerung bietet ([Rhasspy](https://rhasspy.readthedocs.io/#:~:text=voice%20assistant%20services%20%20for,5%20that%20works%20well%20with)). Alternativ gewährleistet **Rhasspy** einen bewährten Offline-Sprachstack für den Pi. Insgesamt erreichen Sie so Ihr Ziel: Eine persönliche KI mit eigenem Namen, eigener Stimme und freier Wissensvermittlung, die Ihnen zuhause zur Hand geht. Viel Erfolg bei der Einrichtung!

**Quellen:** Die Hinweise basieren auf Erfahrungen aus der Smart-Home- und Open-Source-KI-Community, u.a. Home-Assistant-Foren und Projektdokumentationen. Wichtige Referenzen waren: offene STT-/TTS-Projekte (Vosk ([VOSK Offline Speech Recognition API](https://alphacephei.com/vosk/#:~:text=1,but%20there%20are%20much%20bigger)), Piper ([GitHub - rhasspy/piper: A fast, local neural text to speech system](https://github.com/rhasspy/piper#:~:text=A%20fast%2C%20local%20neural%20text,in%20a%20variety%20of%20projects)) ([GitHub - rhasspy/piper: A fast, local neural text to speech system](https://github.com/rhasspy/piper#:~:text=,English%20%28en_GB%2C%20en_US))), Raspberry-Pi-KI-Experimente (LLM auf Pi ([Just for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry Pi 4 : r/RASPBERRY_PI_PROJECTS](https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited))), sowie Home Assistant und Rhasspy Dokus ([How to control Home Assistant with a local LLM instead of ChatGPT | The awesome garage](https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=With%20the%20introduction%20of%20Voice,the%20local%20LLM%20to%20an)) ([Rhasspy](https://rhasspy.readthedocs.io/#:~:text=Rhasspy%20,5%20that%20works%20well%20with)), und das OpenGPT-X Projekt für ein deutschsprachiges Modell ([Multilingual and open source: OpenGPT-X research project releases large language model – Fraunhofer Audio Blog](https://www.audioblog.iis.fraunhofer.com/open-gptx-llm#:~:text=The%20large%20language%20model%20of,and%20has%20a%20distinctly%20European)). Diese belegen die Machbarkeit der vorgeschlagenen Lösung und bieten weiterführende Details. Viel Spaß beim Experimentieren mit Ihrer persönlichen KI!