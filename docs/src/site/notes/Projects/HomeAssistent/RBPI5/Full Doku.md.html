<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Full Doku</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="/ObsidianPublish/docs/style.css?v=2" />
</head>
<body>
<h1
id="leistungsfähige-ki-lösung-für-raspberry-pi-5-empfehlungen-und-umsetzung">Leistungsfähige
KI-Lösung für Raspberry Pi 5 – Empfehlungen und Umsetzung</h1>
<p><strong>Zusammenfassung:</strong> Sie können einen <strong>offenen
Sprachassistenten</strong> auf dem Raspberry Pi 5 realisieren, der
<strong>personalisierbar</strong>, <strong>zweisprachig
(Deutsch/Englisch)</strong>, <strong>offline (keine
Inhaltsfilter)</strong> und in ein <strong>Smart-Home</strong>
integrierbar ist. Dafür benötigt man:</p>
<ul>
<li>Ein <strong>lokal laufendes KI-Sprachmodell</strong> (LLM) mit
ausreichend kleiner Größe (z.B. 7 Milliarden Parameter, quantisiert),
das auf dem Pi läuft und frei konfigurierbar ist (für <strong>eigenen
Namen</strong> und <strong>Antwortstil</strong>).</li>
<li>Eine <strong>Speech-to-Text (STT)</strong> Engine (zur
Spracherkennung Deutsch/Englisch) wie <em>Vosk</em> oder
<em>Whisper</em> auf dem Pi (<a
href="https://alphacephei.com/vosk/#:~:text=1,but%20there%20are%20much%20bigger">VOSK
Offline Speech Recognition API</a>).</li>
<li>Eine <strong>Text-to-Speech (TTS)</strong> Engine (zur Sprachausgabe
mit natürlicher Stimme) wie <em>Piper/Mimic 3</em>, optimiert für
Raspberry Pi (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=Hey%20everyone%21%20I%27ve%20scoured%20the,that%20you%20can%20test%20here">Best
Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top
Choice? : r/RASPBERRY_PI_PROJECTS</a>) (<a
href="https://github.com/rhasspy/piper#:~:text=A%20fast%2C%20local%20neural%20text,in%20a%20variety%20of%20projects">GitHub
- rhasspy/piper: A fast, local neural text to speech system</a>).</li>
<li>Eine <strong>Integrationsplattform</strong> – am einfachsten
<strong>Home Assistant</strong> mit dem neuen Sprachassistent-Framework
(oder alternativ ein offline-Voice-Projekt wie <em>Rhasspy</em>), um die
KI mit Smart-Home-Geräten zu verbinden (<a
href="https://rhasspy.readthedocs.io/#:~:text=Rhasspy%20,5%20that%20works%20well%20with">Rhasspy</a>).</li>
</ul>
<p>Im Folgenden gehen wir auf die empfohlenen Modelle/Frameworks, die
Einrichtung und die geeignete Softwareumgebung im Detail ein.</p>
<h2
id="ki-modell-auswahl-eines-anpassbaren-llm-für-den-raspberry-pi">KI-Modell:
Auswahl eines anpassbaren LLM für den Raspberry Pi</h2>
<p>Für frei formulierte Antworten <strong>ohne inhaltliche
Einschränkungen</strong> bietet sich ein <strong>offenes Large Language
Model</strong> an, das lokal läuft. Wichtig ist dabei die
<strong>Modellgröße</strong>: Ein Raspberry Pi 5 (vorzugsweise mit
<strong>8 GB RAM</strong>) kann ein Modell mit ~6–7 Milliarden
Parametern gerade noch stemmen, wenn es effizient komprimiert ist (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited">Just
for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry
Pi 4 : r/RASPBERRY_PI_PROJECTS</a>) (<a
href="https://www.dfrobot.com/blog-13412.html?srsltid=AfmBOooNjqON_epINpbvnhk4T0QJ0nTjjpX6RPoDUOrJg1nY4qBqxvya#:~:text=Large%20language%20models%20usually%20specify,LLMs%20on%20the%20Raspberry%20Pi">Running
LLM Models (LLaMA, Alpaca, LLaMA2, ChatGLM) on Raspberry Pi 4B for Edge
Computing - DFRobot</a>). Größere Modelle (13B, 30B…) wären zu
ressourcenhungrig. Empfohlene Optionen:</p>
<ul>
<li><p><strong>Meta LLaMA 2 (7B)</strong> oder davon abgeleitete
Modelle: LLaMA 2 ist ein leistungsfähiges Basismodell, das auch für
deutschsprachige Eingaben verwendet werden kann. In quantisierter Form
(z.B. 4-Bit-Quantisierung) benötigt ein 7B-Modell rund 5 GB RAM und
läuft auf dem Pi 5, wenn auch nur mit begrenzter Geschwindigkeit (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited">Just
for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry
Pi 4 : r/RASPBERRY_PI_PROJECTS</a>). Die Antworten kommen <strong>ohne
vorgeschriebene Filter</strong> (Sie steuern die KI komplett selbst).
Durch Anpassung der <strong>Prompts</strong> (Systemnachricht) können
Sie der KI einen Namen geben und den Ton der Antworten festlegen.
Gegebenenfalls lässt sich LLaMA 2 auf Ihrem PC (RTX 3070 Ti)
<strong>feintunen</strong> oder mittels Low-Rank Adaption (LoRA) an
einen gewünschten <strong>Stil</strong> anpassen, und anschließend das
feingetunte Modell auf dem Pi ausführen.</p></li>
<li><p><strong>Open-Assistant/Anthropic-ähnliche Modelle (z.B.
OpenAssistant 7B)</strong>: Das Projekt <strong>OpenAssistant</strong>
hat frei verfügbare Dialogmodelle hervorgebracht, die auf Nutzeranfragen
antworten können. Diese Modelle wurden auf community-generierten
Dialogen (teils mehrsprachig) trainiert und besitzen keine harten
Zensurfilter. Ein 7B-Modell von OpenAssistant kann vergleichbar wie
LLaMA 2 auf dem Pi laufen (ebenfalls mit ~5 GB RAM Bedarf nach
Quantisierung). Vorteil: Sie sind darauf ausgelegt,
<strong>Anweisungen</strong> zu befolgen und könnten natürlicher auf
Fragen reagieren („nach bestem Wissen“). Alternativ gibt es weitere
<strong>RLHF-freie</strong> Modelle (teils als
<em>“uncensored”</em>-Variante bezeichnet), die bewusst ohne
Inhaltsbeschränkung trainiert wurden – diese ließen sich ebenfalls
nutzen, erfordern aber sorgfältigen Umgang.</p></li>
<li><p><strong>Multilinguale Modelle (z.B. Teuken-7B)</strong>: Da Sie
Deutsch <em>und</em> Englisch benötigen, lohnt sich ein Modell, das in
beiden Sprachen kompetent ist. Viele neuere Open-Source-LLMs sind
<strong>mehrsprachig</strong> trainiert. Ein Beispiel ist
<em>Teuken-7B</em> aus dem deutschen <strong>OpenGPT-X</strong> Projekt
– ein 7-Mrd.-Parameter-Modell, das <strong>in allen 24 EU-Sprachen
(inkl. Deutsch und Englisch)</strong> von Grund auf trainiert wurde (<a
href="https://www.audioblog.iis.fraunhofer.com/open-gptx-llm#:~:text=The%20large%20language%20model%20of,and%20has%20a%20distinctly%20European">Multilingual
and open source: OpenGPT-X research project releases large language
model – Fraunhofer Audio Blog</a>) (<a
href="https://www.audioblog.iis.fraunhofer.com/open-gptx-llm#:~:text=Teuken,can%20remain%20within%20the%20company">Multilingual
and open source: OpenGPT-X research project releases large language
model – Fraunhofer Audio Blog</a>). Solche Modelle bringen eine
<em>“europäische”</em> Perspektive mit und sind kommerziell nutzbar. Ein
mehrsprachiges Modell wird Ihre deutschsprachigen Fragen natürlicher
verstehen und beantworten. Alternativ haben auch viele englische
Basismodelle einen gewissen Anteil deutscher Trainingsdaten, sodass
einfache Fragen auf Deutsch oft korrekt beantwortet werden. Zur
Sicherheit kann man das gewählte Modell ggf. mit einigen deutschen
Beispiel-Dialogen nachtrainieren (auf dem PC), um den Stil und die
Verständigkeit weiter zu verbessern.</p></li>
</ul>
<p><strong>Leistung und Anpassung:</strong> Egal für welches Modell Sie
sich entscheiden, der Workflow wäre: <strong>Modellbeschaffung und
-anpassung auf dem PC</strong>, dann <strong>Deployment auf dem
Pi</strong>. Beispielsweise können Sie auf dem PC das Modell
herunterladen und mit Tools wie <em>llama.cpp</em> oder <em>GPT4All</em>
in ein quantisiertes Format konvertieren (4-bit INT4 oder 8-bit) (<a
href="https://www.dfrobot.com/blog-13412.html?srsltid=AfmBOooNjqON_epINpbvnhk4T0QJ0nTjjpX6RPoDUOrJg1nY4qBqxvya#:~:text=with%20smaller%20memory%20footprints,LLMs%20on%20the%20Raspberry%20Pi">Running
LLM Models (LLaMA, Alpaca, LLaMA2, ChatGLM) on Raspberry Pi 4B for Edge
Computing - DFRobot</a>). Testen Sie es zuerst am PC, um
sicherzustellen, dass die gewünschten Persona-Einstellungen (Name, keine
Zensur, Antwortstil) funktionieren. Anschließend übertragen Sie das
kompakte Modell auf den Raspberry Pi. Beachten Sie, dass selbst mit Pi 5
die Antwortzeiten signifikant länger sind als bei PC/GPU – eine 7B-KI
kann auf dem Pi einige Sekunden pro Wort benötigen (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited">Just
for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry
Pi 4 : r/RASPBERRY_PI_PROJECTS</a>). Falls Ihnen das zu langsam ist,
gibt es zwei Ansätze: (1) <strong>noch kleinere Modelle</strong>
einsetzen (z.B. 2–3 Billionen Parameter wie <em>TinyLlama</em> oder
<em>GPT-J 6B</em>), die schneller laufen, oder (2) <strong>Rechenlast
aufteilen</strong> – z.B. den LLM-Teil auf dem PC laufen lassen und nur
STT/TTS auf dem Pi (wenn ein stets verfügbarer PC nicht stört). Für
völlige Autarkie bleibt jedoch ein quantisiertes 7B-Modell auf dem Pi
die beste Kombination aus Leistung und Qualität.</p>
<p><strong>Inhaltliche Freiheiten:</strong> Da die KI offline und unter
Ihrer Kontrolle läuft, können Sie <em>sämtliche</em> inhaltlichen Filter
entfernen oder anpassen. Kommerzielle Assistenten verweigern oft
Antworten aus ethischen Gründen – Ihre eigene KI hingegen kann <em>nach
bestem Wissen und ohne Einschränkungen</em> antworten, solange das
zugrundeliegende Modell die Information enthält. Achten Sie aber darauf,
verantwortungsvoll mit dieser Freiheit umzugehen. Sie können in den
Systemeinstellungen der KI (Initial-Prompt) definieren, dass die KI z.B.
<strong>offen und direkt</strong> antworten soll, und dass sie keine
vordefinierten moralischen Belehrungen geben muss. So erhalten Sie die
gewünschten unzensierten Antworten.</p>
<h2
id="sprachverarbeitung-stt-spracherkennung-und-tts-sprachausgabe">Sprachverarbeitung:
STT (Spracherkennung) und TTS (Sprachausgabe)</h2>
<p>Für eine echte Sprachassistent-Erfahrung müssen Spracheingaben in
Text umgewandelt und Antworten wieder vorgelesen werden. Hier bewähren
sich <strong>offline-fähige STT/TTS-Lösungen</strong> auf dem
Raspberry Pi:</p>
<ul>
<li><p><strong>Speech-to-Text (Spracherkennung)</strong>: Ein sehr gutes
offline STT-Toolkit ist <strong>Vosk</strong> (basierend auf Kaldi).
Vosk bietet vortrainierte Akustikmodelle für <strong>über 20 Sprachen,
darunter Deutsch und Englisch</strong>, und läuft selbst auf schwacher
Hardware <strong>komplett offline (auch auf Raspberry Pi)</strong> (<a
href="https://alphacephei.com/vosk/#:~:text=1,but%20there%20are%20much%20bigger">VOSK
Offline Speech Recognition API</a>). Sie können also mit einem deutschen
oder englischen Satz ins Mikro sprechen, und Vosk liefert Ihnen den
erkannten Text. Die Genauigkeit für deutschsprachige Befehle oder Fragen
ist ordentlich, wenn auch nicht ganz auf dem Niveau moderner
Cloud-Dienste – dafür bleiben die Daten lokal. Eine Alternative ist
<strong>Whisper</strong> (OpenAI), ein neuronales STT-Modell, das sehr
hohe Erkennungsraten erzielt, insbesondere bei Mischsprachen. Allerdings
ist Whisper in den größeren Modellvarianten rechenintensiv; auf dem Pi 5
müsste man die <strong>kleinste Version (Tiny oder Small)</strong>
verwenden, was evtl. noch funktioniert, aber deutlich langsamer ist als
Vosk. (In einer Testumgebung war Whisper <strong>ohne GPU auf dem Pi
extrem langsam</strong> (<a
href="https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=To%20start%20with%2C%20you%20need,GPU%20is%20a%20painful%20experience">How
to control Home Assistant with a local LLM instead of ChatGPT | The
awesome garage</a>).) Für flüssige Interaktion in Echtzeit ist Vosk
daher empfehlenswerter. Vosk lässt sich einfach installieren
(<code>pip3 install vosk</code>) und es stehen kompakte Modelldateien
(~50 MB) für jedes Sprache zur Verfügung (<a
href="https://alphacephei.com/vosk/#:~:text=Tajik%2C%20Telugu,different%20programming%20languages%2C%20too">VOSK
Offline Speech Recognition API</a>) – ideal für den Pi.</p></li>
<li><p><strong>Text-to-Speech (Sprachausgabe)</strong>: Für die Ausgabe
mit einer natürlich klingenden Stimme bietet sich <strong>Piper
(Mimic 3)</strong> an. Piper ist eine <strong>neuronale
TTS-Engine</strong>, die speziell für <em>Edge-Geräte</em> entwickelt
wurde. Sie läuft <strong>lokal in Echtzeit</strong> auf einem
Raspberry Pi 4/5 und liefert hochwertige synthetische Stimmen (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=Hey%20everyone%21%20I%27ve%20scoured%20the,that%20you%20can%20test%20here">Best
Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top
Choice? : r/RASPBERRY_PI_PROJECTS</a>) (<a
href="https://github.com/rhasspy/piper#:~:text=A%20fast%2C%20local%20neural%20text,in%20a%20variety%20of%20projects">GitHub
- rhasspy/piper: A fast, local neural text to speech system</a>). Piper
ist Teil der Open-Source-Mycroft/OVOS-Community (<em>Open Voice OS</em>)
und wird aktiv weiterentwickelt (Open Home Foundation, <em>Year of
Voice</em> Projekt). Es gibt <strong>vorgefertigte Stimmen für viele
Sprachen</strong> – darunter mehrere <strong>deutsche und englische
Stimmen</strong> (<a
href="https://github.com/rhasspy/piper#:~:text=,English%20%28en_GB%2C%20en_US">GitHub
- rhasspy/piper: A fast, local neural text to speech system</a>), die
Sie frei auswählen können. Sie können so die <strong>Stimme der
KI</strong> Ihrer Vorliebe anpassen (z.B. eine weibliche deutsche Stimme
für „Alexa-ähnliches“ Klangbild, oder eine männliche englische Stimme –
je nach Persona). Die Qualität von Piper ist für Offline-TTS
beeindruckend: Das System nutzt neuronale Sprachsynthese (VITS-Modell
als Basis) und erreicht nahezu natürliches Timbre. In Community-Tests
wird Piper 2024 als <strong>führende Offline-TTS-Lösung für den
Pi</strong> angesehen („<strong>Piper ist… schnell, für Raspberry Pi
optimiert und bietet tolle Stimmen in fast jeder Sprache</strong>“ (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=Hey%20everyone%21%20I%27ve%20scoured%20the,that%20you%20can%20test%20here">Best
Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top
Choice? : r/RASPBERRY_PI_PROJECTS</a>)). Installation und Nutzung sind
ebenfalls unkompliziert (es gibt Docker-Container oder man installiert
das Python-Paket). Alternativen wären <em>eSpeak</em> (sehr robotic, nur
als Notfalllösung) oder <em>Coqui-TTS</em> (auch neuronale Stimmen, aber
Piper ist praktisch eine spezialisierte Abspaltung davon). Daher
empfiehlt es sich, Piper/Mimic 3 direkt zu verwenden.</p></li>
</ul>
<blockquote>
<p><strong>Tipp:</strong> Home Assistant bzw. Rhasspy nutzen Piper
bereits als Standard-TTS in ihren Sprachfeatures. Sie können auf der <a
href="https://rhasspy.github.io/piper-samples/">Piper-Projektseite</a>
Audiobeispiele verschiedener Stimmen anhören (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/1c5pxhv/best_offline_texttospeech_tts_for_raspberry_pi_in/#:~:text=best%20solution%20for%202024,that%20you%20can%20test%20here">Best
Offline Text-to-Speech (TTS) for Raspberry Pi in 2024: Is Piper the Top
Choice? : r/RASPBERRY_PI_PROJECTS</a>). Wählen Sie eine Stimme, die
Ihnen gefällt, und laden Sie das entsprechende Sprachmodell (für Deutsch
z.B. die Stimme <em>“de_DE/thorsten”</em> oder <em>“de_DE/elaine”</em>,
für Englisch z.B. <em>“en_US/lessac”</em>). Diese .onnx-Stimmdateien
werden dann vom Piper-Server geladen, um Texte in Sprache
umzuwandeln.</p>
</blockquote>
<ul>
<li><strong>Sprachumschaltung:</strong> Da Ihre KI zweisprachig agieren
soll, müssen STT und TTS <strong>dynamisch die Sprache erkennen bzw.
wechseln</strong> können. Vosk-Engine kann mit separaten Modellen pro
Sprache betrieben werden – Sie könnten anhand der ersten erkannten Worte
entscheiden, ob der Input Deutsch oder Englisch ist, und dann das
passende Spracherkennungsmodell verwenden. (Alternativ gibt es
Multilingual-Modelle für STT, z.B. Whisper kann Sprache automatisch
erkennen – bei Vosk müsste man diesen Schritt selbst vornehmen, z.B.
über einen kurzen <strong>Language Identifier</strong>.) Für TTS könnten
Sie entweder eine zweisprachige Stimme nutzen (falls verfügbar) oder je
nach Sprache der Antwort eine deutsche bzw. englische Stimme wählen.
Piper hat z.B. für Englisch und Deutsch separate Stimmen, aber Sie
können dem System beibringen: <em>„Wenn Antworttext Deutsch, dann
deutsche Stimme nutzen.“</em> Diese Logik lässt sich im
Integrations-Code umsetzen. Das KI-Modell selbst wird anhand der Frage
vermutlich auch in der entsprechenden Sprache antworten. (Falls nicht,
kann man im Prompt vorgeben: <em>„Beantworte deutsche Fragen auf Deutsch
und englische auf Englisch.“</em>) Insgesamt ist so sichergestellt, dass
die KI Sie versteht und in passender Sprache antwortet.</li>
</ul>
<h2
id="personalisierung-name-stimme-und-antwortstil-der-ki">Personalisierung:
Name, Stimme und Antwortstil der KI</h2>
<p>Ein großer Vorteil einer eigenen KI-Lösung ist die <strong>freie
Personalisierung</strong>. Sie können Ihrer KI einen individuellen
<strong>Namen</strong> geben, die <strong>Stimmfarbe</strong> anpassen
und den <strong>Tonfall der Antworten</strong> konfigurieren:</p>
<ul>
<li><p><strong>Name und Anrede:</strong> Der Name der KI ist rein
konfigurationsseitig – Sie können ihn frei wählen (z.B. „Ada“, „Jarvis“,
„Heimhelfer“, etc.). Im Sprachgebrauch können Sie diesen Namen als
<strong>Wake Word</strong> verwenden, falls Sie einen Hotword-Listener
einsetzen (z.B. “Hey Jarvis” als Signal zum Zuhören).
Offline-Wakeword-Erkennung kann z.B. mit <em>Porcupine</em> oder Mycroft
<strong>Precise</strong> realisiert werden, wo Sie ein eigenes Keyword
trainieren können. Alternativ nutzen Sie einfach einen Knopfdruck oder
Trigger in Home Assistant, um das Mikro zu aktivieren – dann ist der
Name mehr für die <strong>Ansprache in Antworten</strong> relevant. Sie
können dem KI-Modell im System-Prompt mitteilen: <em>„Du heißt [Name]
und sprichst den Nutzer mit ‚du/Sie‘ an.“</em> So kann die KI ihren
Namen ggf. in ihren Sätzen verwenden (oder Sie skripten es, dass die KI
z.B. sagt „[Name] steht zu Diensten.“ o.ä.). Da das Modell komplett
unter Ihrer Kontrolle ist, können Sie solche Persona-Details frei
festlegen.</p></li>
<li><p><strong>Stimme:</strong> Die Stimme wird durch das TTS-System
bestimmt. Mit Piper/Mimic3 können Sie aus zahlreichen Stimmen auswählen
(<a
href="https://github.com/rhasspy/piper#:~:text=,English%20%28en_GB%2C%20en_US">GitHub
- rhasspy/piper: A fast, local neural text to speech system</a>). Für
eine persönliche Note könnten Sie sogar Ihre <em>eigene Stimme</em>
klonen – das erfordert jedoch das Training eines eigenen
TTS-Stimmenmodells (mit einigen Minuten Sprachaufnahmen und Training am
PC). Einfacher ist die Wahl einer bestehenden sympathischen Stimme.
Beispielsweise gibt es eine deutsche männliche Stimme (<em>Thorsten</em>
– recht natürlich klingend) und eine weibliche (<em>Eva</em> oder
<em>Birgit</em>). Für Englisch sind Stimmen wie <em>LibriVox</em> oder
<em>Lessac</em> verfügbar. Die Stimme können Sie jederzeit austauschen
oder für verschiedene Sprachen unterschiedliche Sprecher nutzen. So
bekommt Ihre KI einen eindeutigen Charakterklang.</p></li>
<li><p><strong>Antwortstil:</strong> Der Sprachstil der Antworten lässt
sich auf <strong>mehreren Ebenen</strong> anpassen. Erstens können Sie
im <strong>Prompting</strong> dem KI-Modell Anweisungen geben, z.B.:
<em>„Antworte stets höflich und fachkundig, aber ruhig locker. Du darfst
auch humorvoll sein.“</em> oder <em>„Verwende bei Antworten auf Deutsch
eher informelle Sprache (Du) und auf Englisch einen freundlichen
Ton.“</em> Diese Systeminstruktionen wirken wie eine
Persönlichkeitsschablone für das Modell. Zweitens können Sie durch
<strong>Fine-Tuning</strong> die KI an einen bestimmten Stil gewöhnen –
z.B. indem Sie Beispiel-Dialoge im gewünschten Ton trainieren. Drittens
entscheidet auch das konkrete Modell: einige Modelle (z.B.
OpenAssistant) neigen zu ausführlichen, möglicherweise lockeren
Antworten; andere (z.B. Alpaca) antworten sehr prägnant. Wählen Sie ein
Modell, dessen „Stimmcharakter“ Ihren Vorstellungen entspricht, oder
passen Sie es an. Da Sie keine Filter haben, können Sie auch kontroverse
oder sarkastische Stile zulassen, wenn gewünscht. Wichtig: Falls Sie ein
bereits <strong>RLHF-gefeintes</strong> Modell wie LLaMA 2 Chat nutzen,
könnten dort noch milde Inhaltsfilter aktiv sein (die Modelle wurden
darauf trainiert, beleidigende Inhalte zu vermeiden). Diese können Sie
überschreiben, indem Sie im Prompt klarstellen, dass die KI <em>alles
beantworten</em> soll. Notfalls greifen Sie zu einem Basismodell ohne
RLHF oder einem explizit „ungefilterten“ Community-Finetune.</p></li>
</ul>
<p>Zusammengefasst ist <strong>Personalisierung</strong> eine Stärke
dieser DIY-KI: Von Name über Stimme bis Antwortverhalten können Sie
alles Ihren Wünschen entsprechend einstellen. Vieles davon erfordert nur
Konfiguration (keine Programmierung) – z.B. den Namen in einem Skript
ändern, eine andere TTS-Stimme wählen oder ein paar Beispielsätze als
Vorlage geben, damit die KI den gewünschten Ton trifft.</p>
<h2
id="integration-ins-smart-home-home-assistant-vs.-alternativen">Integration
ins Smart Home: Home Assistant vs. Alternativen</h2>
<p>Um die KI praktisch einzusetzen, insbesondere für
Smart-Home-Steuerung, braucht es eine
<strong>Software-Umgebung</strong>, die <strong>Mikrofon</strong>,
<strong>Lautsprecher</strong>, <strong>KI-Logik</strong> und
<strong>Haussteuerung</strong> zusammenbringt. Hier bieten sich zwei
Hauptwege an:</p>
<p><strong>1. Home Assistant mit Voice Assistant Pipeline:</strong></p>
<p>Home Assistant (HA) ist eine mächtige Smart-Home-Zentrale und hat
kürzlich umfangreiche Sprachassistenz-Funktionen erhalten (Stichwort
<em>“Year of Voice”</em> bei HA). Ab <strong>Home Assistant
2023.x</strong> können Sie eigene <strong>Voice Pipelines</strong>
konfigurieren – das heißt, Sie können <strong>frei wählen, welche
STT-Engine, KI und TTS-Engine</strong> genutzt werden sollen, sowie ein
Wakeword (<a
href="https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=With%20the%20introduction%20of%20Voice,the%20local%20LLM%20to%20an">How
to control Home Assistant with a local LLM instead of ChatGPT | The
awesome garage</a>). HA liefert damit den Orchestrierungsrahmen: Sobald
Sie einen Sprachbefehl geben, wandelt HA ihn per STT in Text, gibt ihn
an einen <strong>Conversation Agent</strong> (Ihre KI) weiter, erhält
die Antwort als Text und liest sie per TTS vor. Dies alles kann lokal
geschehen, wenn die entsprechenden Module lokal angebunden sind. Sie
könnten z.B. in der <code>configuration.yaml</code> von HA einen
<strong>Conversation-Agent</strong> vom Typ „OpenAI“ eintragen, der aber
auf einen lokalen Endpoint zeigt (dazu später mehr). Für STT wählen Sie
<strong>Vosk</strong> oder <strong>Whisper</strong> (es gibt
Community-Addons bzw. Integrationen dafür), für TTS wählen Sie
<strong>Piper</strong> (ist in HA inzwischen integriert oder als Add-on
verfügbar). Ihr Raspberry Pi 5 könnte all das ausführen – ggf. in Form
von Docker-Containern, die HA orchestriert.</p>
<p>Die Integration mit dem Smart Home ist in HA am reibungslosesten:
Ihre KI kann direkt <strong>Geräte schalten</strong> oder
<strong>Sensoren abfragen</strong>, wenn Sie entsprechende Intents
definieren. Es gibt zwei Möglichkeiten, wie die KI mit HA-Devices
interagieren kann: (a) über <strong>vordefinierte Absichten</strong>
(Intents), die z.B. bestimmte Phrasen („Licht an im Wohnzimmer“)
erkennen und dann einen HA-Befehl auslösen, oder (b) über die neue
<strong>Funktionen-API</strong> von OpenAI, die Home Assistant
unterstützt – dabei kann das KI-Modell sogenannte <em>function
calls</em> auslösen. Ein entsprechend trainiertes Modell (oder ein
Prompt mit Funktionenschema) könnte z.B. bei der Anfrage <em>„Schalte
das Wohnzimmerlicht auf 50%.“</em> intern eine JSON-Funktion
<code>light.turn_on</code> an HA schicken. Tatsächlich experimentiert
die Community mit solchen Ansätzen; im Blog-Beispiel wurde ein
<strong>LLM-Modell verwendet, das Home-Assistant-Funktionen
versteht</strong> (<a
href="https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=When%20you%27ve%20gotten%20Whisper%20and,to%20run%20on%20your%20GPU">How
to control Home Assistant with a local LLM instead of ChatGPT | The
awesome garage</a>). Das ist allerdings fortgeschritten –
einsteigertauglicher ist es, Intents per HA’s eigenem Intent-System
abzubilden und die KI für freies Fragen/Antworten daneben laufen zu
lassen.</p>
<p><strong>Warum Home Assistant?</strong> Wenn Sie ohnehin ein HA-System
betreiben, bietet es sich an, dieses als <em>Zentralplattform</em> zu
nutzen. HA gewährleistet, dass alle Komponenten (Mikrofon, Lautsprecher,
KI, Geräte) zusammenspielen. Zudem bleibt Ihre Lösung
<strong>erweiterbar</strong> – Sie können zukünftige HA-Updates nutzen,
die evtl. lokale LLM-Integration noch einfacher machen. (Die Entwickler
von HA arbeiten aktiv an lokalen Sprachsteuerungsfunktionen.) Im Zweifel
lässt sich auch ein Teil der Pipeline auf dem PC auslagern, während HA
auf dem Pi läuft – z.B. könnte <strong>Whisper auf dem PC</strong>
laufen und HA per Netzwerk dessen STT-Ergebnis nutzen (<a
href="https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=blazing%20fast%20performance,enough%20to%20follow%20this%20tutorial">How
to control Home Assistant with a local LLM instead of ChatGPT | The
awesome garage</a>), oder das LLM auf dem PC gehostet und HA schickt die
Prompts dorthin. HA ist flexibel genug für solche verteilten Setups.</p>
<p><strong>2. Alternativen: Rhasspy oder andere
Offline-Assistant-Frameworks:</strong></p>
<p>Falls Sie <strong>nicht</strong> das gesamte Home-Assistant-Framework
nutzen möchten (oder bereits eine andere Smarthome-Zentrale haben), gibt
es spezialisierte <strong>Offline-Sprachassistenten</strong>.
<strong>Rhasspy</strong> ist ein bekanntes Open-Source-Projekt, das
genau dies bietet: Eine <strong>vollständig
Offline-Sprachassistent-Plattform für viele Sprachen</strong>, lauffähig
auf Raspberry Pi (<a
href="https://rhasspy.readthedocs.io/#:~:text=Rhasspy%20,5%20that%20works%20well%20with">Rhasspy</a>).
Rhasspy wurde als Nachfolger von Snips.ai entwickelt und unterstützt
ebenfalls <strong>Home Assistant, Node-RED, OpenHAB</strong> u.a.
Systeme out-of-the-box (<a
href="https://rhasspy.readthedocs.io/#:~:text=voice%20assistant%20services%20%20for,5%20that%20works%20well%20with">Rhasspy</a>).
Man definiert bei Rhasspy <em>Sprachbefehle mittels Vorlagen</em>
(Grammatik), und Rhasspy erkennt diese und sendet JSON-Events, um
Aktionen auszulösen (<a
href="https://rhasspy.readthedocs.io/#:~:text=You%20specify%20voice%20commands%20in,a%20template%20language">Rhasspy</a>).
Für Ihr Vorhaben würde Rhasspy den Part von STT (z.B. mit eigenen
Wakeword, Vosk STT) und TTS (Piper) übernehmen und die
Smart-Home-Befehle direkt an Home Assistant weitergeben.
<strong>Vorteil:</strong> Sehr ausgereifte Offline-Sprachsteuerung,
<strong>Nachteil:</strong> Freie Konversation (offene Fragen an die KI)
sind standardmäßig nicht vorgesehen. Allerdings lässt sich Rhasspy
erweitern – man kann einen <em>Fallback-Intent</em> definieren, der alle
Eingaben auffängt, die keinem Befehl zugeordnet wurden, und diesen Text
dann an Ihr KI-Modell schickt. Die KI-Antwort könnte Rhasspy dann wieder
per TTS ausgeben. So erhalten Sie neben den vordefinierten
Haussteuerungs-Kommandos auch spontane Frage-Antwort-Fähigkeiten.
Rhasspy erlaubt viel Feintuning und läuft stabil auf dem Pi; es ist
jedoch etwas komplexer in der Ersteinrichtung als HA (man konfiguriert
Profile, muss ggf. Audio-Hardware einrichten, etc.). Wenn Sie bereits HA
nutzen, ist wahrscheinlich der direkte HA-Weg einfacher. Wenn Sie aber
lieber eine <strong>entkoppelte Sprachsteuerung</strong> haben wollen
(die notfalls ohne HA-Kern funktioniert) oder mehrere
<em>Sprach-Satelliten</em> im Haus (mehrere Pis mit Mics in
verschiedenen Räumen) betreiben möchten, dann ist Rhasspy hervorragend
geeignet – es unterstützt sogar verteilte Installationen
(Satellit/Nachricht broker).</p>
<p><strong>3. Weitere Optionen:</strong> Neben Rhasspy gibt es
<strong>OpenVoiceOS (OVOS)</strong> bzw. <strong>Neon AI</strong>, Forks
des Mycroft-Assistenten. Diese bieten komplette Sprachassistenz mit
Skill-System. Man könnte z.B. einen OVOS auf dem Pi 5 laufen lassen, der
mittels einer <em>ChatGPT-Skill</em> (hier dann auf Ihr lokales LLM
umgebogen) allgemeine Fragen beantwortet, und mittels einer
<em>HomeAssistant-Skill</em> Geräte steuert. OVOS hat
Personalisierungs-Features (kann benannt werden, hat TTS/STT plugins
etc.). Allerdings ist Mycroft/OVOS relativ schwergewichtig und noch in
Entwicklung – für Bastler aber einen Blick wert. Ebenfalls möglich ist
die Nutzung von <strong>Node-RED</strong> Flows, um STT-&gt;LLM-&gt;TTS
Ketten zu bauen, oder Sprachmodule in anderen Smart-Home-Lösungen
(OpenHAB z.B. kann externe Skripte aufrufen). Insgesamt scheint jedoch
<strong>Home Assistant mit seinen neuen Voice-Fähigkeiten die
naheliegendste und zukunftssichere Umgebung</strong> für Ihr Vorhaben zu
sein.</p>
<h2 id="einrichtung-und-anpassung-vorgehensweise">Einrichtung und
Anpassung: Vorgehensweise</h2>
<p>Zum Abschluss hier ein mögliches <strong>Vorgehensschema</strong>, um
die KI auf dem Raspberry Pi 5 zum Laufen zu bringen und ins Smart Home
zu integrieren:</p>
<ol type="1">
<li><p><strong>Modellvorbereitung auf dem PC:</strong> Installieren Sie
auf Ihrem leistungsfähigen PC die benötigten Tools (z.B.
<code>transformers</code> oder <code>llama.cpp</code>), und laden Sie
das gewünschte Basismodell herunter (z.B. LLaMA 2 7B oder Teuken-7B).
Führen Sie ggf. ein <strong>Finetuning oder LoRA-Merge</strong> durch,
um persönliche Anpassungen (z.B. bilinguales Verhalten, kein Filter,
bestimmter Gesprächston) einzubringen. Anschließend
<strong>quantisieren</strong> Sie das Modell, um es zu verkleinern –
etwa mit <em>GPTQ</em> oder <em>llama.cpp quantize</em>. Ziel ist ein
Modell, das &lt;6 GB Speicher belegt. Testen Sie das quantisierte Modell
auf dem PC einmal mit ein paar deutschen und englischen Fragen, um
sicher zu sein, dass es korrekt funktioniert und Ihren Anforderungen
entspricht (z.B. keine ungewollten Refusals).</p></li>
<li><p><strong>Raspberry Pi einrichten:</strong> Sorgen Sie dafür, dass
der Pi 5 eine geeignete Linux-Distribution laufen hat (z.B. Raspberry Pi
OS 64-Bit). Installieren Sie die nötigen Abhängigkeiten für Audio
(Mikrofon und Lautsprecher sollten am Pi funktionieren). Kopieren Sie
das quantisierte Modell zum Pi. Installieren Sie dann:</p>
<ul>
<li><strong>STT</strong>: z.B. <code>vosk</code> Python-Bibliothek und
laden Sie das deutsche und englische Modell für Vosk. (Alternativ
Whisper.cpp, falls ausprobiert.)</li>
<li><strong>TTS</strong>: z.B. Piper – entweder via Docker (es gibt ein
<a href="https://hub.docker.com/r/rhasspy/piper">rhasspy/piper
Docker-Image</a>) oder durch Kompilieren der ONNX Runtime auf ARM. Laden
Sie die gewünschten Stimmenpakete (de_DE und en_US).</li>
<li><strong>LLM Runtime</strong>: Installieren Sie
<code>llama.cpp</code> auf dem Pi (lässt sich für ARM kompilieren) oder
nutzen Sie ein leichtgewichtiges Serving-Framework wie
<strong>LocalAI</strong>. <em>LocalAI</em> ist ein Open-Source-Projekt,
das eine API bereitstellt, welche kompatibel zur OpenAI-API ist, aber
lokal Ihre Modelle ausführt (<a
href="https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=When%20you%27ve%20gotten%20Whisper%20and,to%20run%20on%20your%20GPU">How
to control Home Assistant with a local LLM instead of ChatGPT | The
awesome garage</a>). Damit könnten Sie z.B. Home Assistant vorgaukeln,
es spräche mit der OpenAI API, tatsächlich aber antwortet LocalAI mit
Ihrem lokalen Modell. LocalAI kann per Docker auf dem Pi laufen (es
unterstützt CPU-Betrieb mit quantisierten Modellen). Alternativ können
Sie auch ein eigenes Python-Skript schreiben, das Texteingaben an
llama.cpp weiterleitet – für HA-Integration ist aber die API-Methode
bequemer.</li>
</ul></li>
<li><p><strong>Verknüpfung der Komponenten:</strong> Wenn Sie Home
Assistant verwenden, richten Sie dort eine <strong>Conversation
Pipeline</strong> ein. Beispiel: Wake Word (optional) -&gt; STT (Vosk)
-&gt; Conversation Agent (LocalAI mit Ihrem Modell) -&gt; TTS (Piper).
In der HA-Dokumentation bzw. Community finden sich Beispiele, wie man
benutzerdefinierte STT/TTS einbindet und einen eigenen Conversational
Agent konfiguriert. Oft läuft es darauf hinaus, in den Einstellungen von
HA einen externen URL für STT/TTS anzugeben (wo z.B. ein Piper-Server
lauscht) und einen <strong>“ChatGPT”</strong>-ähnlichen Agenten zu
konfigurieren, der aber auf LocalAI zeigt. Sobald dies steht, können Sie
per Sprache mit Ihrem HA-System sprechen. Sagen Sie z.B. <em>„Wie ist
das Wetter heute in Berlin?“</em>, dann sollte die Pipeline greifen: Ihr
Satz wird von Vosk erkannt, als Text an das LLM geschickt, dieses
generiert eine Antwort, und Piper gibt die Antwortstimme aus. –
<strong>Tipp:</strong> Testen und debuggen Sie schrittweise: Erst STT
allein (Mikrofon-Test), dann TTS (eine Testnachricht sprechen lassen),
dann die KI-Textantwort via HA’s Text-Eingabe (ohne Sprache) überprüfen,
und schließlich alles verkoppeln.</p></li>
<li><p><strong>Smart-Home-Steuerung einbinden:</strong> Bringen Sie
Ihrer KI bei, auch Geräte zu steuern. Der einfache Weg: Definieren Sie
in Home Assistant <strong>Sprachbefehle/Intents</strong> für typische
Aktionen (Licht, Thermostat, etc.). HA kann solche Befehle auch ohne LLM
verstehen (per Regex oder Synonym-Liste). Die KI muss also nicht jeden
Befehl selbst „wissen“. Stattdessen fangen die HA-Intents bestimmte
Schlüsselwörter ab und schalten direkt die Geräte. Für frei formulierte
Befehle können Sie die KI nutzen: Z.B. Frage: <em>„Mir ist kalt.“</em>
-&gt; KI erkennt evtl., dass die Heizung höher gestellt werden soll –
Sie könnten das KI-Antwort-Template so gestalten, dass sie in solchen
Fällen eine Funktion ausführt oder einen speziellen JSON zurückgibt, den
HA auswertet. Dies ist allerdings komplex. Einfacher: Entweder nur feste
Sprachbefehle fürs Smart Home (und KI nur für generelle Fragen),
<strong>oder</strong> KI-Antworten nicht nur als Text behandeln, sondern
parsen. Die Home-Assistant-Community entwickelt hier laufend Lösungen,
sodass Ihre lokale KI Kontext über Ihr Smart Home hat.</p></li>
<li><p><strong>Feinschliff und Testing:</strong> Zum Schluss justieren
Sie die Persona-Details. Experimentieren Sie mit verschiedenen
Prompt-Vorlagen, bis Ihnen der Ton gefällt. Wechseln Sie die TTS-Stimme,
falls die Verständlichkeit oder Persönlichkeit noch nicht passt. Testen
Sie bilinguale Dialoge – z.B. erst eine Frage auf Deutsch, dann eine auf
Englisch – und prüfen Sie, ob die KI korrekt umschaltet. Möglicherweise
müssen Sie in Ihrem Pipeline-Code die Sprache erkennen und dem KI-Modell
mitteilen. All das können Sie nach und nach verbessern. Da alles lokal
läuft, haben Sie die volle Kontrolle und keine Abhängigkeit von
Cloud-APIs.</p></li>
</ol>
<p><strong>Fazit:</strong> Mit einem Raspberry Pi 5 und der richtigen
Auswahl an Open-Source-Komponenten können Sie einen <em>hochgradig
anpassbaren Sprachassistenten</em> umsetzen. Ein quantisiertes
<strong>LLM (ca. 7 B Parameter)</strong> läuft auf dem Pi und liefert
intelligente Antworten in Deutsch und Englisch, <strong>ohne
Zensur</strong>. Kombiniert mit <strong>Vosk</strong> (offline-STT) und
<strong>Piper</strong> (offline-TTS) bleibt die Lösung komplett lokal.
Für die Smart-Home-Integration ist <strong>Home Assistant</strong>
empfehlenswert, da es bereits Hooks für STT/TTS und Gerätesteuerung
bietet (<a
href="https://rhasspy.readthedocs.io/#:~:text=voice%20assistant%20services%20%20for,5%20that%20works%20well%20with">Rhasspy</a>).
Alternativ gewährleistet <strong>Rhasspy</strong> einen bewährten
Offline-Sprachstack für den Pi. Insgesamt erreichen Sie so Ihr Ziel:
Eine persönliche KI mit eigenem Namen, eigener Stimme und freier
Wissensvermittlung, die Ihnen zuhause zur Hand geht. Viel Erfolg bei der
Einrichtung!</p>
<p><strong>Quellen:</strong> Die Hinweise basieren auf Erfahrungen aus
der Smart-Home- und Open-Source-KI-Community, u.a. Home-Assistant-Foren
und Projektdokumentationen. Wichtige Referenzen waren: offene
STT-/TTS-Projekte (Vosk (<a
href="https://alphacephei.com/vosk/#:~:text=1,but%20there%20are%20much%20bigger">VOSK
Offline Speech Recognition API</a>), Piper (<a
href="https://github.com/rhasspy/piper#:~:text=A%20fast%2C%20local%20neural%20text,in%20a%20variety%20of%20projects">GitHub
- rhasspy/piper: A fast, local neural text to speech system</a>) (<a
href="https://github.com/rhasspy/piper#:~:text=,English%20%28en_GB%2C%20en_US">GitHub
- rhasspy/piper: A fast, local neural text to speech system</a>)),
Raspberry-Pi-KI-Experimente (LLM auf Pi (<a
href="https://www.reddit.com/r/RASPBERRY_PI_PROJECTS/comments/194xney/just_for_fun_running_the_llama2_gpt_and_speech/#:~:text=%E2%80%A2%20%E2%80%A2%20Edited">Just
for fun: running the LLaMA-2 GPT and Speech recognition on the Raspberry
Pi 4 : r/RASPBERRY_PI_PROJECTS</a>)), sowie Home Assistant und Rhasspy
Dokus (<a
href="https://theawesomegarage.com/blog/configure-a-local-llm-to-control-home-assistant-instead-of-chatgpt#:~:text=With%20the%20introduction%20of%20Voice,the%20local%20LLM%20to%20an">How
to control Home Assistant with a local LLM instead of ChatGPT | The
awesome garage</a>) (<a
href="https://rhasspy.readthedocs.io/#:~:text=Rhasspy%20,5%20that%20works%20well%20with">Rhasspy</a>),
und das OpenGPT-X Projekt für ein deutschsprachiges Modell (<a
href="https://www.audioblog.iis.fraunhofer.com/open-gptx-llm#:~:text=The%20large%20language%20model%20of,and%20has%20a%20distinctly%20European">Multilingual
and open source: OpenGPT-X research project releases large language
model – Fraunhofer Audio Blog</a>). Diese belegen die Machbarkeit der
vorgeschlagenen Lösung und bieten weiterführende Details. Viel Spaß beim
Experimentieren mit Ihrer persönlichen KI!</p>
</body>
</html>
